var documenterSearchIndex = {"docs":
[{"location":"references/#references-section","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Modules = [HMatrices]","category":"page"},{"location":"references/#Base.Matrix-Tuple{Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}}","page":"References","title":"Base.Matrix","text":"Matrix(H::HMatrix;global_index=true)\n\nConvert H to a Matrix. If global_index=true (the default), the entries are given in the global indexing system (see HMatrix for more information); otherwise the local indexing system induced by the row and columns trees are used.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.AbstractCompressor","page":"References","title":"HMatrices.AbstractCompressor","text":"abstract type AbstractCompressor\n\nTypes used to compress matrices.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.AbstractKernelMatrix","page":"References","title":"HMatrices.AbstractKernelMatrix","text":"abstract type AbstractKernelMatrix{T} <: AbstractMatrix{T}\n\nInterface for abstract matrices represented through a kernel function f, target elements X, and source elements Y. The matrix entry i,j is given by f(X[i],Y[j]). Concrete subtypes should implement at least\n\n`Base.getindex(K::AbstractKernelMatrix,i::Int,j::Int)`\n\nIf a more efficient implementation of getindex(K,I::UnitRange,I::UnitRange), getindex(K,I::UnitRange,j::Int) and getindex(adjoint(K),I::UnitRange,j::Int) is available (e.g. with SIMD vectorization), implementing such methods can improve the speed of assembling an HMatrix.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.AbstractSplitter","page":"References","title":"HMatrices.AbstractSplitter","text":"abstract type AbstractSplitter\n\nAn AbstractSplitter is used to split a ClusterTree. The interface requires the following methods:\n\nshould_split(clt,splitter) : return a Bool determining if the ClusterTree should be further divided\nsplit!(clt,splitter) : perform the splitting of the ClusterTree handling the necessary data sorting.\n\nSee GeometricSplitter for an example of an implementation.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.CardinalitySplitter","page":"References","title":"HMatrices.CardinalitySplitter","text":"struct CardinalitySplitter <: AbstractSplitter\n\nUsed to split a ClusterTree along the largest dimension if length(tree)>nmax. The split is performed so the data is evenly distributed amongst all children.\n\nSee also: AbstractSplitter\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.ClusterTree","page":"References","title":"HMatrices.ClusterTree","text":"ClusterTree(elements,splitter;[copy_elements=true, threads=false])\n\nConstruct a ClusterTree from the  given elements using the splitting strategy encoded in splitter. If copy_elements is set to false, the elements argument are directly stored in the ClusterTree and are permuted during the tree construction.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.ClusterTree-2","page":"References","title":"HMatrices.ClusterTree","text":"mutable struct ClusterTree{N,T}\n\nTree structure used to cluster poitns of type SVector{N,T} into HyperRectangles.\n\nFields:\n\n_elements::Vector{SVector{N,T}} : vector containing the sorted elements.\ncontainer::HyperRectangle{N,T} : container for the elements in the current node.\nindex_range::UnitRange{Int} : indices of elements contained in the current node.\nloc2glob::Vector{Int} : permutation from the local indexing system to the original (global) indexing system used as input in the construction of the tree.\nglob2loc::Vector{Int} : inverse of loc2glob permutation.\nchildren::Vector{ClusterTree{N,T}}\nparent::ClusterTree{N,T}\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.DHMatrix","page":"References","title":"HMatrices.DHMatrix","text":"mutable struct DHMatrix{R,T} <: AbstractMatrix{T}\n\nConcrete type representing a hierarchical matrix with data distributed amongst various workers. Its structure is very similar to HMatrix, except that the leaves store a RemoteHMatrix object.\n\nThe data on the leaves of a DHMatrix may live on a different worker, so calling fetch on them should be avoided whenever possible.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.DHMatrix-Union{Tuple{T}, Tuple{R}, Tuple{R, R}} where {R, T}","page":"References","title":"HMatrices.DHMatrix","text":"DHMatrix{T}(rowtree,coltree;partition_strategy=:distribute_columns)\n\nConstruct the block structure of a distributed hierarchical matrix covering rowtree and coltree. Returns a DHMatrix with leaves that are empty.\n\nThe partition_strategy keyword argument determines how to partition the blocks for distributed computing. Currently, the only available options is distribute_columns, which will partition the columns of the underlying matrix into floor(log2(nw)) parts, where nw is the number of workers available.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.GeometricSplitter","page":"References","title":"HMatrices.GeometricSplitter","text":"struct GeometricSplitter <: AbstractSplitter\n\nUsed to split a ClusterTree in half along the largest axis. The children boxes are shrank to tighly fit the data.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.HMatrix","page":"References","title":"HMatrices.HMatrix","text":"mutable struct HMatrix{R,T} <: AbstractMatrix{T}\n\nA hierarchial matrix constructed from a rowtree and coltree of type R and holding elements of type T.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.HMatrix-Union{Tuple{T}, Tuple{R}, Tuple{R, R, Any}} where {R, T}","page":"References","title":"HMatrices.HMatrix","text":"HMatrix{T}(rowtree,coltree,adm)\n\nConstruct an empty HMatrix with rowtree and coltree using the admissibility condition adm. This function builds the skeleton for the hierarchical matrix, but does not compute data field in the blocks. See assemble_hmatrix for assembling a hierarhical matrix.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.HyperRectangle","page":"References","title":"HMatrices.HyperRectangle","text":"struct HyperRectangle{N,T}\n\nAxis-aligned hyperrectangle in N dimensions given by low_corner::SVector{N,T} and high_corner::SVector{N,T}.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.KernelMatrix","page":"References","title":"HMatrices.KernelMatrix","text":"KernelMatrix{Tf,Tx,Ty,T} <:: AbstractKernelMatrix{T}\n\nGeneric kernel matrix representing a kernel function acting on two sets of elements. If K is a KernelMatrix, then K[i,j] = f(X[i],Y[j]) where f::Tf=kernel(K), X::Tx=rowelements(K) and Y::Ty=colelements(K).\n\nExamples\n\nX = rand(SVector{2,Float64},2)\nY = rand(SVector{2,Float64},2)\nK = KernelMatrix(X,Y) do x,y\n    sum(x+y)\nend\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.MulLinearOp","page":"References","title":"HMatrices.MulLinearOp","text":"struct MulLinearOp{R,T} <: AbstractMatrix{T}\n\nAbstract matrix representing the following linear operator:\n\n    L = R + P + a * ∑ᵢ Aᵢ * Bᵢ\n\nwhere R and P are of type RkMatrix{T}, Aᵢ,Bᵢ are of type HMatrix{R,T} and a is scalar multiplier. Calling compressor(L) produces a low-rank approximation of L, where compressor is an AbstractCompressor.\n\nNote: this structure is used to group the operations required when multiplying hierarchical matrices so that they can later be executed in a way that minimizes recompression of intermediate computations.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.PartialACA","page":"References","title":"HMatrices.PartialACA","text":"struct PartialACA\n\nAdaptive cross approximation algorithm with partial pivoting. This structure can be used to generate an RkMatrix from a matrix-like object M as follows:\n\nusing LinearAlgebra\nrtol = 1e-6\ncomp = PartialACA(;rtol)\nA = rand(10,2)\nB = rand(10,2)\nM = A*adjoint(B) # a low-rank matrix\nR = comp(M, axes(M)...) # compress the entire matrix `M`\nnorm(Matrix(R) - M) < rtol*norm(M) # true\n\n# output\n\ntrue\n\n\nBecause it uses partial pivoting, the linear operator does not have to be evaluated at every i,j. This is usually much faster than TSVD, but due to the pivoting strategy the algorithm may fail in special cases, even when the underlying linear operator is of low rank.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.PermutedMatrix","page":"References","title":"HMatrices.PermutedMatrix","text":"PermutedMatrix{K,T} <: AbstractMatrix{T}\n\nStructured used to reprensent the permutation of a matrix-like object. The original matrix is stored in the data::K field, and the permutations are stored in rowperm and colperm.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.PrincipalComponentSplitter","page":"References","title":"HMatrices.PrincipalComponentSplitter","text":"struct PrincipalComponentSplitter <: AbstractSplitter\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.RemoteHMatrix","page":"References","title":"HMatrices.RemoteHMatrix","text":"struct RemoteHMatrix{S,T}\n\nA light wrapper for a Future storing an HMatrix.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.RkMatrix","page":"References","title":"HMatrices.RkMatrix","text":"mutable struct RkMatrix{T}\n\nRepresentation of a rank r matrix M in outer product format M = A*adjoint(B) where A has size m × r and B has size n × r.\n\nThe internal representation stores A and B, but R.Bt or R.At can be used to get the respective adjoints.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.StrongAdmissibilityStd","page":"References","title":"HMatrices.StrongAdmissibilityStd","text":"struct StrongAdmissibilityStd\n\nTwo blocks are admissible under this condition if the minimum of their diameter is smaller than eta times the distance between them, where eta::Float64 is a parameter.\n\nUsage:\n\nadm = StrongAdmissibilityStd(;eta=2.0)\nadm(Xnode,Ynode)\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.TSVD","page":"References","title":"HMatrices.TSVD","text":"struct TSVD\n\nCompression algorithm based on a posteriori truncation of an SVD. This is the optimal approximation in Frobenius norm; however, it also tends to be very expensive and thus should be used mostly for \"small\" matrices.\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.VectorOfVectors","page":"References","title":"HMatrices.VectorOfVectors","text":"struct VectorOfVectors{T}\n\nA simple structure which behaves as a Vector{Vector{T}} but stores the entries in a contiguous data::Vector{T} field. All vectors in the VectorOfVectors are assumed to be of size m, and there are k of them, meaning this structure can be used to represent a m × k matrix.\n\nSimilar to a vector-of-vectors, calling A[i] returns a view to the i-th column.\n\nSee also: newcol!\n\n\n\n\n\n","category":"type"},{"location":"references/#HMatrices.WeakAdmissibilityStd","page":"References","title":"HMatrices.WeakAdmissibilityStd","text":"struct WeakAdmissibilityStd\n\nTwo blocks are admissible under this condition if the distance between them is positive.\n\n\n\n\n\n","category":"type"},{"location":"references/#Base.split-Union{Tuple{N}, Tuple{HyperRectangle{N}, Any, Any}} where N","page":"References","title":"Base.split","text":"split(rec::HyperRectangle,[axis]::Int,[place])\n\nSplit a hyperrectangle in two along the axis direction at the  position place. Returns a tuple with the two resulting hyperrectangles.\n\nWhen no place is given, defaults to splitting in the middle of the axis.\n\nWhen no axis and no place is given, defaults to splitting along the largest axis.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._aca_partial","page":"References","title":"HMatrices._aca_partial","text":"_aca_partial(K,irange,jrange,atol,rmax,rtol,istart=1)\n\nInternal function implementing the adaptive cross-approximation algorithm with partial pivoting. The returned R::RkMatrix provides an approximation to K[irange,jrange] which has either rank is expected to satisfy|M - R| < max(atol,rtol*|M|)`, but this inequality may fail to hold due to the various errors involved in estimating the error and |M|.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices._aca_partial_pivot-Tuple{Any, Any}","page":"References","title":"HMatrices._aca_partial_pivot","text":"_aca_partial_pivot(v,I)\n\nFind in the valid set I the index of the element x ∈ v maximizing its smallest singular value. This is equivalent to minimizing the spectral norm of the inverse of x.\n\nWhen x is a scalar, this is simply the element with largest absolute value.\n\nThis general implementation should work for both scalar as well as tensor-valued kernels; see (https://www.sciencedirect.com/science/article/pii/S0021999117306721)[https://www.sciencedirect.com/science/article/pii/S0021999117306721] for more details.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._assemble_cpu!-NTuple{4, Any}","page":"References","title":"HMatrices._assemble_cpu!","text":"_assemble_cpu!(hmat::HMatrix,K,comp)\n\nAssemble data on the leaves of hmat. The admissible leaves are compressed using the compressor comp. This function assumes the structure of hmat has already been intialized, and therefore should not be called directly. See HMatrix information on constructors.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._assemble_hmat_distributed-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._assemble_hmat_distributed","text":"_assemble_hmat_distributed(K,rtree,ctree;adm=StrongAdmissibilityStd(),comp=PartialACA();global_index=true,threads=false)\n\nInternal methods called after the DHMatrix structure has been initialized in order to construct the HMatrix on each of the leaves of the DHMatrix.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._assemble_threads!-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._assemble_threads!","text":"_assemble_threads!(hmat::HMatrix,K,comp)\n\nLike _assemble_cpu!, but uses threads to assemble the leaves. Note that the threads are spanwned using Threads.@spawn, which means they are spawned on the same worker as the caller.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._build_block_structure!-Union{Tuple{T}, Tuple{R}, Tuple{Any, HMatrix{R, T}}} where {R, T}","page":"References","title":"HMatrices._build_block_structure!","text":"_build_block_structure!(adm_fun,current_node)\n\nRecursive constructor for HMatrix block structure. Should not be called directly.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._hgemv_recursive!-Tuple{AbstractVector, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, AbstractVector, Any}","page":"References","title":"HMatrices._hgemv_recursive!","text":"_hgemv_recursive!(C,A,B,offset)\n\nInternal function used to compute C[I] <-- C[I] + A*B[J] where I = rowrange(A) - offset[1] and J = rowrange(B) - offset[2].\n\nThe offset argument is used on the caller side to signal if the original hierarchical matrix had a pivot other than (1,1).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices._update_frob_norm-Tuple{Any, Any, Any}","page":"References","title":"HMatrices._update_frob_norm","text":"_update_frob_norm(acc,A,B)\n\nGiven the Frobenius norm of Rₖ = A[1:end-1]*adjoint(B[1:end-1]) in acc, compute the Frobenius norm of Rₖ₊₁ = A*adjoint(B) efficiently.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.assemble_hmatrix-Tuple{AbstractKernelMatrix}","page":"References","title":"HMatrices.assemble_hmatrix","text":"assembel_hmatrix(K::AbstractKernelMatrix[; atol, rank, rtol, kwargs...])\n\nConstruct an approximation of K as an HMatrix using the partial ACA algorithm for the low rank blocks. The atol, rank, and rtol optional arguments are passed to the PartialACA constructor, and the remaining keyword arguments are forwarded to the main assemble_hmatrix function.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.assemble_hmatrix-Union{Tuple{T}, Tuple{Type{T}, Any, Any, Any}} where T","page":"References","title":"HMatrices.assemble_hmatrix","text":"assemble_hmatrix([T,], K, rowtree, coltree;\n    adm=StrongAdmissibilityStd(),\n    comp=PartialACA(),\n    threads=true,\n    distributed=false,\n    global_index=true)\n\nMain routine for assembling a hierarchical matrix. The argument K represents the matrix to be approximated, rowtree and coltree are tree structure partitioning the row and column indices, respectively, adm can be called on a node of rowtree and a node of coltree to determine if the block is compressible, and comp is a function/functor which can compress admissible blocks.\n\nIt is assumed that K supports getindex(K,i,j), and that comp can be called as comp(K,irange::UnitRange,jrange::UnitRange) to produce a compressed version of K[irange,jrange] in the form of an RkMatrix.\n\nThe type paramter T is used to specify the type of the entries of the matrix, by default is inferred from K using eltype(K).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.binary_split!-Union{Tuple{T}, Tuple{N}, Tuple{ClusterTree{N, T}, Function}} where {N, T}","page":"References","title":"HMatrices.binary_split!","text":"binary_split!(cluster::ClusterTree,predicate)\n\nSplit a ClusterTree into two, sorting all elements in the process according to predicate. cluster is assigned as parent to each children.\n\nEach point is sorted according to whether f(x) returns true (point sorted on the \"left\" node) or false (point sorted on the \"right\" node). At the end a minimal HyperRectangle containing all left/right points is created.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.center-Tuple{HyperRectangle}","page":"References","title":"HMatrices.center","text":"center(Ω)\n\nCenter of the smallest ball containing Ω.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compress!-Tuple{HMatrices.RkMatrix, TSVD}","page":"References","title":"HMatrices.compress!","text":"compress!(M::RkMatrix,tsvd::TSVD)\n\nRecompress the matrix R using a truncated svd of R. The implementation uses the qr-svd strategy to efficiently compute svd(R) when rank(R) ≪ min(size(R)).\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compress!-Tuple{Matrix, TSVD}","page":"References","title":"HMatrices.compress!","text":"compress!(M::Matrix,tsvd::TSVD)\n\nRecompress the matrix M using a truncated svd and output an RkMatrix. The data in M is invalidated in the process.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.compression_ratio-Tuple{Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}}","page":"References","title":"HMatrices.compression_ratio","text":"compression_ratio(H::HTypes)\n\nThe ratio of the uncompressed size of H to its compressed size. A compression_ratio of 10 means it would have taken 10 times more memory to store H as a dense matrix.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.container-Tuple{ClusterTree}","page":"References","title":"HMatrices.container","text":"container(clt::ClusterTree)\n\nReturn the object enclosing all the elements of the clt.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.depth","page":"References","title":"HMatrices.depth","text":"depth(tree,acc=0)\n\nRecursive function to compute the depth of node in a a tree-like structure.\n\nOverload this function if your structure has a more efficient way to compute depth (e.g. if it stores it in a field).\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.diameter-Tuple{HyperRectangle}","page":"References","title":"HMatrices.diameter","text":"diameter(Ω)\n\nLargest distance between x and y for x,y ∈ Ω.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.distance-Tuple{ClusterTree, ClusterTree}","page":"References","title":"HMatrices.distance","text":"distance(X::ClusterTree, Y::ClusterTree)\n\nDistance between the containers of X and Y.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.distance-Union{Tuple{N}, Tuple{HyperRectangle{N}, HyperRectangle{N}}} where N","page":"References","title":"HMatrices.distance","text":"distance(Ω1,Ω2)\n\nMinimal Euclidean distance between a point x ∈ Ω1 and y ∈ Ω2.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.elements-Tuple{ClusterTree}","page":"References","title":"HMatrices.elements","text":"elements(clt::ClusterTree)\n\nIterable list of the elements inside clt.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.filter_tree","page":"References","title":"HMatrices.filter_tree","text":"filter_tree(f,tree,isterminal=true)\n\nReturn a vector containing all the nodes of tree such that f(node)==true.  The argument isterminal can be used to control whether to continue the search on children of nodes for which f(node)==true.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.filter_tree!","page":"References","title":"HMatrices.filter_tree!","text":"filter_tree!(filter,nodes,tree,[isterminal=true])\n\nLike filter_tree, but appends results to nodes.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.getblock!-NTuple{4, Any}","page":"References","title":"HMatrices.getblock!","text":"getblock!(block,K,irange,jrange)\n\nFill block with K[i,j] for i ∈ irange, j ∈ jrange, where block is of size length(irange) × length(jrange).\n\nA default implementation exists which relies on getindex(K,i,j), but this method can be overloaded for better performance if e.g. a vectorized way of computing a block is available.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.getcol!-Tuple{Any, Any, Any}","page":"References","title":"HMatrices.getcol!","text":"getcol!(col, M, j)\n\nReturn the j-th column of M in col.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.getcol!-Union{Tuple{T}, Tuple{Any, HMatrices.RkMatrix, Int64}, Tuple{Any, HMatrices.RkMatrix, Int64, Val{T}}} where T","page":"References","title":"HMatrices.getcol!","text":"getcol!(col,M::AbstractMatrix,j)\n\nFill the entries of col with column j of M.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.getcol-Tuple{Any, Any}","page":"References","title":"HMatrices.getcol","text":"getcol(M, j)\n\nReturn the j-th column of M.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.glob2loc-Tuple{ClusterTree}","page":"References","title":"HMatrices.glob2loc","text":"glob2loc(clt::ClusterTree)\n\nThe inverse of loc2glob.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.hmul!-Union{Tuple{T}, Tuple{T, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, Any, Any, Any}, Tuple{T, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, Vararg{Any, 4}}, Tuple{T, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, Union{HMatrix, LinearAlgebra.Adjoint{T, HMatrix{R, T}} where {R, T}, LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}, Vararg{Any, 5}}} where T<:HMatrix","page":"References","title":"HMatrices.hmul!","text":"hmul!(C::HMatrix,A::HMatrix,B::HMatrix,a,b,compressor)\n\nSimilar to mul! : compute C <-- A*B*a + B*b, where A,B,C are hierarchical matrices and compressor is a function/functor used in the intermediate stages of the multiplication to avoid growring the rank of admissible blocks after addition is performed.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.index_range-Tuple{ClusterTree}","page":"References","title":"HMatrices.index_range","text":"index_range(clt::ClusterTree)\n\nIndices of elements in root_elements(clt) which lie inside clt.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.isclean-Tuple{HMatrix}","page":"References","title":"HMatrices.isclean","text":"isclean(H::HMatrix)\n\nReturn true if all leaves of H have data, and if the leaves are the only nodes containing data. This is the normal state of an ℋ-matrix, but during intermediate stages of a computation data may be associated with non-leaf nodes for convenience.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.leaves-Tuple{Any}","page":"References","title":"HMatrices.leaves","text":"leaves(tree)\n\nReturn a vector containing all the leaf nodes of tree.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.loc2glob-Tuple{ClusterTree}","page":"References","title":"HMatrices.loc2glob","text":"loc2glob(clt::ClusterTree)\n\nThe permutation from the (local) indexing system of the elements of the clt to the (global) indexes used upon the construction of the tree.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.newcol!-Tuple{HMatrices.VectorOfVectors}","page":"References","title":"HMatrices.newcol!","text":"newcol!(A::VectorOfVectors)\n\nAppend a new (unitialized) column to A, and return a view of it.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.nodes-Tuple{Any}","page":"References","title":"HMatrices.nodes","text":"leaves(tree)\n\nReturn a vector containing all the nodes of tree.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.parentnode-Tuple{ClusterTree}","page":"References","title":"HMatrices.parentnode","text":"parentnode(clt::ClusterTree)\n\nThe node's parent. If t is a root, then parent(t)==t.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.radius-Tuple{HyperRectangle}","page":"References","title":"HMatrices.radius","text":"radius(Ω)\n\nHalf the diameter.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.reset!-Tuple{HMatrices.VectorOfVectors}","page":"References","title":"HMatrices.reset!","text":"reset!(A::VectorOfVectors)\n\nSet the number of columns of A to zero, and the number of rows to zero, but does not resize! the underlying data vector.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.root_elements-Tuple{ClusterTree}","page":"References","title":"HMatrices.root_elements","text":"root_elements(clt::ClusterTree)\n\nThe elements contained in the root of the tree to which clt belongs.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.should_split","page":"References","title":"HMatrices.should_split","text":"should_split(clt::ClusterTree, depth, splitter::AbstractSplitter)\n\nDetermine whether or not a ClusterTree should be further divided.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.split!","page":"References","title":"HMatrices.split!","text":"split!(clt::ClusterTree,splitter::AbstractSplitter)\n\nDivide clt using the strategy implemented by splitter. This function is reponsible of assigning the children and parent fields, as well as of permuting the data of clt.\n\n\n\n\n\n","category":"function"},{"location":"references/#HMatrices.use_global_index-Tuple{}","page":"References","title":"HMatrices.use_global_index","text":"use_global_index()::Bool\n\nDefault choice of whether operations will use the global indexing system throughout the package.\n\n\n\n\n\n","category":"method"},{"location":"references/#HMatrices.use_threads-Tuple{}","page":"References","title":"HMatrices.use_threads","text":"use_threads()::Bool\n\nDefault choice of whether threads will be used throughout the package.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.cholesky!-Tuple{LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}, Any}","page":"References","title":"LinearAlgebra.cholesky!","text":"cholesky!(M::HMatrix,comp)\n\nHierarhical cholesky facotrization of M, using comp to generate the compressed blocks during the multiplication routines.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.cholesky!-Tuple{LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}}","page":"References","title":"LinearAlgebra.cholesky!","text":"cholesky!(M::HMatrix;atol=0,rank=typemax(Int),rtol=atol>0 ||\nrank<typemax(Int) ? 0 : sqrt(eps(Float64)))\n\nHierarhical cholesky facotrization of M, using the PartialACA(;atol,rtol;rank) compressor.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.cholesky-Tuple{LinearAlgebra.Hermitian{T, HMatrix{R, T}} where {R, T}, Vararg{Any}}","page":"References","title":"LinearAlgebra.cholesky","text":"cholesky(M::HMatrix,args...;kwargs...)\n\nHierarchical cholesky factorization. See cholesky! for the available options.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.lu!-Tuple{HMatrix, Any}","page":"References","title":"LinearAlgebra.lu!","text":"lu!(M::HMatrix,comp)\n\nHierarhical LU facotrization of M, using comp to generate the compressed blocks during the multiplication routines.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.lu!-Tuple{HMatrix}","page":"References","title":"LinearAlgebra.lu!","text":"lu!(M::HMatrix;atol=0,rank=typemax(Int),rtol=atol>0 ||\nrank<typemax(Int) ? 0 : sqrt(eps(Float64)))\n\nHierarhical LU facotrization of M, using the PartialACA(;atol,rtol;rank) compressor.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.lu-Tuple{HMatrix, Vararg{Any}}","page":"References","title":"LinearAlgebra.lu","text":"LinearAlgebra.lu(M::HMatrix,args...;kwargs...)\n\nHierarchical LU factorization. See lu! for the available options.\n\n\n\n\n\n","category":"method"},{"location":"references/#LinearAlgebra.mul!","page":"References","title":"LinearAlgebra.mul!","text":"mul!(y::AbstractVector,H::HMatrix,x::AbstractVector,a,b[;global_index,threads])\n\nPerform y <-- H*x*a + y*b in place.\n\n\n\n\n\n","category":"function"},{"location":"benchs/#Benchmark-Report-for-*HMatrices*","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"","category":"section"},{"location":"benchs/#Job-Properties","page":"Benchmark Report for HMatrices","title":"Job Properties","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Time of benchmark: 11 Jun 2022 - 16:25\nPackage commit: 007341\nJulia commit: bf5349\nJulia command flags: -O3\nEnvironment variables: JULIA_NUM_THREADS => 4 OPEN_BLAS_NUM_THREADS => 1","category":"page"},{"location":"benchs/#Results","page":"Benchmark Report for HMatrices","title":"Results","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Below is a table of this job's results, obtained by running the benchmarks. The values listed in the ID column have the structure [parent_group, child_group, ..., key], and can be used to index into the BaseBenchmarks suite to retrieve the corresponding benchmarks. The percentages accompanying time and memory values in the below table are noise tolerances. The \"true\" time/memory value for a given benchmark is expected to fall within this percentage of the reported value. An empty cell means that the value was zero.","category":"page"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"ID time GC time memory allocations\n[\"Compressors\", \"ACA(0.0, 9223372036854775807, 1.0e-6)\"] 60.896 ms (5%) 16.845 ms 114.87 MiB (1%) 70\n[\"Compressors\", \"PartialACA(0.0, 9223372036854775807, 1.0e-6)\"] 215.466 μs (5%)  451.02 KiB (1%) 46\n[\"Compressors\", \"TSVD(0.0, 9223372036854775807, 1.0e-6)\"] 344.458 ms (5%) 8.569 ms 46.04 MiB (1%) 17\n[\"Helmholtz\", \"assemble cpu\"] 96.266 s (5%) 5.449 s 19.36 GiB (1%) 713824\n[\"Helmholtz\", \"assemble threads\"] 24.179 s (5%) 96.356 ms 19.36 GiB (1%) 736860\n[\"Helmholtz\", \"gemv cpu\"] 949.268 ms (5%)  3.05 MiB (1%) 6\n[\"Helmholtz\", \"gemv threads\"] 181.576 ms (5%)  9.16 MiB (1%) 55\n[\"Helmholtz\", \"lu\"] 66.773 s (5%) 763.845 ms 57.89 GiB (1%) 8146509\n[\"HelmholtzVec\", \"assemble cpu\"] 69.864 s (5%) 1.057 s 17.76 GiB (1%) 1092683\n[\"HelmholtzVec\", \"assemble threads\"] 20.562 s (5%) 310.865 ms 17.76 GiB (1%) 1116981\n[\"Laplace\", \"assemble cpu\"] 2.493 s (5%) 161.411 ms 4.52 GiB (1%) 315818\n[\"Laplace\", \"assemble threads\"] 604.592 ms (5%)  4.53 GiB (1%) 338851\n[\"Laplace\", \"gemv cpu\"] 272.634 ms (5%)  2.29 MiB (1%) 6\n[\"Laplace\", \"gemv threads\"] 76.399 ms (5%)  5.34 MiB (1%) 55\n[\"Laplace\", \"lu\"] 26.139 s (5%) 1.065 s 26.82 GiB (1%) 7637938\n[\"LaplaceVec\", \"assemble cpu\"] 2.300 s (5%)  4.11 GiB (1%) 343964\n[\"LaplaceVec\", \"assemble threads\"] 740.280 ms (5%)  4.11 GiB (1%) 366996","category":"page"},{"location":"benchs/#Benchmark-Group-List","page":"Benchmark Report for HMatrices","title":"Benchmark Group List","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Here's a list of all the benchmark groups executed by this job:","category":"page"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"[\"Compressors\"]\n[\"Helmholtz\"]\n[\"HelmholtzVec\"]\n[\"Laplace\"]\n[\"LaplaceVec\"]","category":"page"},{"location":"benchs/#Julia-versioninfo","page":"Benchmark Report for HMatrices","title":"Julia versioninfo","text":"","category":"section"},{"location":"benchs/","page":"Benchmark Report for HMatrices","title":"Benchmark Report for HMatrices","text":"Julia Version 1.7.2\nCommit bf53498635 (2022-02-06 15:21 UTC)\nPlatform Info:\n  OS: Linux (x86_64-pc-linux-gnu)\n      Ubuntu 20.04.4 LTS\n  uname: Linux 5.13.0-28-generic #31~20.04.1-Ubuntu SMP Wed Jan 19 14:08:10 UTC 2022 x86_64 x86_64\n  CPU: Intel(R) Xeon(R) W-2145 CPU @ 3.70GHz: \n                 speed         user         nice          sys         idle          irq\n       #1-16  1200 MHz    1843921 s       7865 s      84009 s  1574372975 s          0 s\n       \n  Memory: 251.4216766357422 GB (207606.51171875 MB free)\n  Uptime: 9.85318111e6 sec\n  Load Avg:  2.4  1.4  0.98\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-12.0.1 (ORCJIT, skylake-avx512)","category":"page"},{"location":"notebooks/#notebook-section","page":"Notebooks","title":"Notebooks","text":"","category":"section"},{"location":"notebooks/","page":"Notebooks","title":"Notebooks","text":"The following links provide some useful examples/tutorial in the format of notebooks. You need to download the notebook, and then open it with Pluto.jl in order to interact with it.","category":"page"},{"location":"notebooks/","page":"Notebooks","title":"Notebooks","text":"Vectorized kernels","category":"page"},{"location":"dhmatrix/#dhmatrix-section","page":"Distributed HMatrix","title":"Distributed hierarchical matrix","text":"","category":"section"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"warning: Warning\nThis is still an experimental feature!","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"When calling assemble_hmatrix, the keyword argument distributed can be set to true in order to generate a DHMatrix object. The main difference between an HMatrix and a DHMatrix is that the leaves of a DHMatrix represent a remote reference to an HMatrix possibly stored on a different worker. ","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"In order to use the distributed capabilities, you must first add the Distributed package and add some workers:","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"using Distributed\naddprocs(4)","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"You can then load the HMatrices package everywhere, and proceed as before:","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"@everywhere using HMatrices\nusing LinearAlgebra, StaticArrays\nconst Point3D = SVector{3,Float64}\nm = 10_000\nX = Y = [Point3D(sin(θ)cos(ϕ),sin(θ)*sin(ϕ),cos(θ)) for (θ,ϕ) in zip(π*rand(m),2π*rand(m))]\nconst μ = 5\nfunction G(x,y)\n    r = x-y\n    d = norm(r) + 1e-10\n    1/(8π*μ) * (1/d*I + r*transpose(r)/d^3)\nend\nK = KernelMatrix(G,X,Y)\nH = assemble_hmatrix(K;atol=1e-4,distributed=true,threads=false)","category":"page"},{"location":"dhmatrix/","page":"Distributed HMatrix","title":"Distributed HMatrix","text":"TODO: add an interactive notebook example ","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"CurrentModule = HMatrices","category":"page"},{"location":"#home-section","page":"Getting started","title":"HMatrices.jl","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"A package for assembling and factoring hierarchical matrices","category":"page"},{"location":"#Overview","page":"Getting started","title":"Overview","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"This package provides some functionality for assembling as well as for doing linear algebra with hierarchical matrices. The main structure exported is the HMatrix type, which can be used to efficiently approximate certain linear operators containing a hierarchical low-rank structure. Once assembled, a hierarchical matrix can be used to accelerate the solution of Ax=b in a variety of ways. Below you will find a quick introduction for how to assemble and utilize an HMatrix; see the References section for more information on the available methods and structures.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nAlthough hierarchical matrices have a broad range of application, this package focuses on their use to approximate integral operators arising in boundary integral equation (BIE) methods. As such, most of the API has been designed with BIEs in mind, and the examples that follow will focus on the compression of integral operators. Feel free to open an issue or reach out if you have an interesting application of hierarchical matrices in mind not covered by this package!","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Useful references\nThe notation and algorithms implemented were mostly drawn from the following references:Hackbusch, Wolfgang. Hierarchical matrices: algorithms and analysis. Vol. 49. Heidelberg: Springer, 2015.\nBebendorf, Mario. Hierarchical matrices. Springer Berlin Heidelberg, 2008.","category":"page"},{"location":"#assemble-generic-subsection","page":"Getting started","title":"Assembling an HMatrix","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"In order to assemble an HMatrix, you need the following (problem-specific) ingredients:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The matrix-like object K that you wish to compress\nA rowtree and coltree providing a hierarchical partition of the rows and columns of K\nAn admissibility condition for determining (a priory) whether a block given by a node in the rowtree and node in the coltree is compressible\nA function/functor to generate a low-rank approximation of compressible blocks","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"To illustrate how this is done for a concrete problem, consider two set of points X = left boldsymbolx_i right_i=1^m and Y =leftboldsymbolx_j right_j=1^n in mathbbR^3, and let K be a  m times n matrix with entries given by:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"  K_ij = G(boldsymbolx_iboldsymboly_j)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"for some kernel function G. To make things simple, we will take X and Y to be points distributed on a circle:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using Random # hide\nRandom.seed!(1) # hide\nusing HMatrices, LinearAlgebra, StaticArrays\nconst Point2D = SVector{2,Float64}\n\n# points on a circle\nm = n = 10_000\nX = Y = [Point2D(sin(i*2π/n),cos(i*2π/n)) for i in 0:n-1]\nnothing","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Next we will create the matrix-like structure to represent the object K. We will pick G to be the free-space Greens function for Laplace's equation in two-dimensions:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"struct LaplaceMatrix <: AbstractMatrix{Float64}\n  X::Vector{Point2D}  \n  Y::Vector{Point2D}\nend\n\nBase.getindex(K::LaplaceMatrix,i::Int,j::Int) = -1/2π*log(norm(K.X[i] - K.Y[j]) + 1e-10)\nBase.size(K::LaplaceMatrix) = length(K.X), length(K.Y)\n\n# create the abstract matrix\nK = LaplaceMatrix(X,Y)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The next step consists in partitioning the point clouds X and Y into a tree-like data structure so that blocks corresponding to well-separated points can be easily distinguished and compressed. The WavePropBase package provides the ClusterTree struct for this purpose (see its documentation for more details on available options):","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Xclt = Yclt = ClusterTree(X)\nnothing # hide","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The object Xclt represents a tree partition of the point cloud into axis-aligned bounding boxes.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The third requirement is an admissibilty condition to determine if the interaction between two clusters should be compressed. We will use the StrongAdmissibilityStd, which is appropriate for asymptotically smooth kernels such as the one considered:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"adm = StrongAdmissibilityStd()\nnothing # hide","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The final step is to provide a method to compress admissible blocks. Here we will use the PartialACA functor implementing an adaptive cross approximation with partial pivoting strategy:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"comp = PartialACA(;atol=1e-6)\nnothing # hide","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"With these ingredients at hand, we can assemble an approximation for K using","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"H = assemble_hmatrix(K,Xclt,Yclt;adm,comp,threads=false,distributed=false)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"important: Important\nThe assemble_hmatrix function is the main constructor exported by this package, so it is worth getting familiar with it and the various keyword arguments it accepts.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nReasonable defaults exist for the admissibility condition, cluster tree, and compressor when the kernel K is an AbstractKernelMatrix, so that the construction process is somewhat simpler than just presented in those cases. Manually constructing each ingredient, however, gives a level of control not available through the default constructors. See the Kernel matrices section for more details.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"You can now use H in lieu of K (as an approximation) for certain linear algebra operations, as shown next.","category":"page"},{"location":"#Matrix-vector-product-and-iterative-solvers","page":"Getting started","title":"Matrix vector product and iterative solvers","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"The simplest operation you can perform with an HMatrix is to multiply it by a vector:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"x = rand(n)\nnorm(H*x - K*x)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"More advanced options (such as choosing between a threaded or serial implementation) can be accessed by calling mul! directly:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"y = similar(x)\nmul!(y,H,x,1,0;threads=false)\nnorm(y - K*x)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The example below illustrates how to use the HMatrix H constructed above with the IterativeSolvers package:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using IterativeSolvers\nb = rand(m)\napprox = gmres!(y,H,b;abstol=1e-6)\nexact  = Matrix(K)\\b\nnorm(approx-exact)/norm(exact)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Internally, the hierarchical matrix H is stored as H = inv(Pr)*_H*Pc, where Pr and Pc are row and column permutation matrices induced by the clustering of the target and source points as ClusterTrees, respectively, and _H is the actual hierarchical matrix constructed. It is sometimes convenient to work directly with _H for performance reasons; for example, in the iterative solver above, you may want to permute rows and columns only once offline and perform the matrix multiplication with _H. The keyword argument global_index=false can be passed to perform the desired operations on _H instead, or you may overload the HMatrices.use_global_index method which will in turn change the default value of global_index throughout the package (but be careful to know what you are doing, as this may cause some unexpected results); similarly, you can overload HMatrices.use_threads to globally change whether threads are used by default. In the iterative example above, for instance, we may permute the vectors externally before and after (but not in each forward product) as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"cperm  = HMatrices.colperm(H) # column permutation\nrperm  = HMatrices.rowperm(H) # row permutation\nbp     = b[cperm] \nHMatrices.use_global_index() = false # perform operations on the local indexing system\napprox = gmres!(y,H,bp;abstol=1e-6)\ninvpermute!(approx,rperm)\nHMatrices.use_global_index() = true # go back to default\nnorm(approx-exact)/norm(exact)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Problem size\nFor \"small\" problem sizes, the overhead associated with the more complex structure of an HMatrix will lead to computational times that are larger than the dense representation, even when the HMatrix occupies less memory. For large problem sizes, however, the loglinear complexity will yield significant gains in terms of memory and cpu time provided the underlying operator has a hierarchical low-rank structure.","category":"page"},{"location":"#Factorization-and-direct-solvers","page":"Getting started","title":"Factorization and direct solvers","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Although the forward map illustrated in the example above suffices to solve the linear system Kx = b using an iterative solver, there are circumstances where a direct solver is desirable (because, e.g., the system is not well-conditioned or you wish to solve it for many right-hand-sides b). At present, the only available factorization is the hierarchical lu factorization of H, which can be accomplished as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"F = lu(H;atol=1e-6)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Note that unliked the matrix-vector product, factoring H is not exact in the sense that lu(H) ≠ lu(Matrix(H)). The accuracy of the approximation can be controlled through the keyword arguments atol,rol and rank, which are used in the various intermediate truncations performed during the factorization. See lu for more details.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"important: Truncation error\nThe parameters atol and rtol are used to control the truncation of low-rank blocks adaptively using an estimate of the true error (in Frobenius norm). These local errors may accumulate after successive truncations, meaning that the global approximation error (in Frobenius norm) may be larger than the prescribed tolerance.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The returned object F is of the LU type, and efficient routines are provided to solve linear system using F:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"approx = F\\b\nnorm(approx-exact)/norm(exact)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Note that the error in solving the linear system may be significantly larger than the error in computing H*x due to the condition of the underlying operator.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nBecause factoring an HMatrix with a small error tolerance can be quite time-consuming, a hybrid strategy commonly employed consists of using a rough factorization (with e.g. large tolerance or a fixed rank) as a preconditioner to an iterative solver.","category":"page"},{"location":"#Other-kernels","page":"Getting started","title":"Other kernels","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"So far we focused on the (manual) compression of a simple kernel matrix where the entry (i,j) depended only on a function G and on point-clouds X and Y. There are many interesting applications where computing the (i,j) entry requires more information, such as triangles, basis functions, or normal vectors. To illustrate how the methods discussed before could be adapted lets construct a double-layer matrix for Laplace equation. To keep the example simple, we will re-use the point clouds X and Y defined before, so that we do not have to reconstruct the target and source cluster trees, and we will simply append the normal vector information to a LaplaceDoubleLayer structure. The implementation could look something like this:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"struct LaplaceDoubleLayer <: AbstractMatrix{Float64}\n    X::Vector{Point2D}\n    Y::Vector{Point2D}\n    NY::Vector{Point2D} # normals at Y coordinate\nend\n\nfunction Base.getindex(K::LaplaceDoubleLayer,i::Int,j::Int) \n    r = K.X[i] - K.Y[j]\n    d = norm(r) + 1e-10\n    return (1 / (2π) / (d^2) * dot(r, K.NY[j]))\nend\nBase.size(K::LaplaceDoubleLayer) = length(K.X), length(K.Y)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We can now simply instantiate a double-layer kernel, and compress it as before","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"# create the abstract matrix\nny = Y\nK = LaplaceDoubleLayer(X,Y,ny)\nH = assemble_hmatrix(K,Xclt,Yclt;adm,comp,threads=false,distributed=false)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"With H assembled, everything else works exactly as before!","category":"page"},{"location":"#Index","page":"Getting started","title":"Index","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"","category":"page"},{"location":"kernelmatrix/#kernelmatrix-section","page":"Kernel matrices","title":"Kernel matrices","text":"","category":"section"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"While in the introduction we presented a somewhat general way to assemble an HMatrix, abstract matrices associated with an underlying kernel function are common enough in boundary integral equations that a special interface exists for facilitating their use.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"The AbstractKernelMatrix interface is used to represent matrices K with i,j entry given by f(X[i],Y[j]), where X=rowelements(K) and Y=colelements(K). The row and columns elements may be as simple as points in mathbbR^d (as is the case for Nyström methods), but they can also be more complex objects such as triangles or basis functions. In such cases, it is required that center(X[i]) and center(Y[j]) return a point as an SVector, and that f(X[i],Y[j]) make sense.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"A concrete implementation of AbstractKernelMatrix is provided by the KernelMatrix type. Creating the matrix associated with the Helmholtz free-space Greens function, for example, can be accomplished through:","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"using HMatrices, LinearAlgebra, StaticArrays\nconst Point3D = SVector{3,Float64}\nX = rand(Point3D,10_000)\nY = rand(Point3D,10_000)\nconst k = 2π\nfunction G(x,y) \n    d = norm(x-y)\n    exp(im*k*d)/(4π*d)\nend\nK = KernelMatrix(G,X,Y)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"Compressing K is now as simple as:","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"H = assemble_hmatrix(K;rtol=1e-6)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"It is worth noting that several default choices are made during the compression above. See the Assembling an HMatrix section or the documentation of assemble_hmatrix for information on how to obtain a more granular control of the assembling stage.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"As before, you can multiply H by a vector, or do an lu factorization of it.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"Finally, here is a somewhat contrived example of how to use a KernelMatrix when the rowelements and colelements are not simply points (as required e.g. in a Galerkin discretization of boundary integral equations):","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"using HMatrices, LinearAlgebra, StaticArrays\n# create a simple structure to represent a segment\nstruct Segment\n    start::SVector{2,Float64}\n    stop::SVector{2,Float64}\nend\n# extend the function center to work with segments\nHMatrices.center(s::Segment) = 0.5*(s.start + s.stop)\n# P1 mesh of a circle\nnpts = 10_000\nnodes = [SVector(cos(s), sin(s)) for s in range(0,stop=2π,length=npts+1)]\nsegments = [Segment(nodes[i],nodes[i+1]) for i in 1:npts]\n# Now define a kernel function that takes two segments (instead of two points) and returns a scalar\nfunction G(target::Segment,source::Segment) \n    x, y = HMatrices.center(target), HMatrices.center(source)\n    d = norm(x-y)\n    return -log(d + 1e-10)\nend\nK = KernelMatrix(G,segments,segments)\n# compress the kernel matrix\nH = assemble_hmatrix(K;rtol=1e-6)","category":"page"},{"location":"kernelmatrix/#Support-for-tensor-kernels","page":"Kernel matrices","title":"Support for tensor kernels","text":"","category":"section"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"warning: Warning\nSupport for tensor-valued kernels should be considered experimental at this stage.","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"For vector-valued partial differential equations such as Stokes or time-harmonic Maxwell's equation, the underlying integral operator has a kernel function which is a tensor. This package currently provides some limited support for these types of operators. The example below illustrates how to build an HMatrix representing a KernelMatrix corresponding to Stokes Greens function for points on a sphere:","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"using HMatrices, LinearAlgebra, StaticArrays\nconst Point3D = SVector{3,Float64}\nm = 5_000\nX = Y = [Point3D(sin(θ)cos(ϕ),sin(θ)*sin(ϕ),cos(θ)) for (θ,ϕ) in zip(π*rand(m),2π*rand(m))]\nconst μ = 5\nfunction G(x,y)\n    r = x-y\n    d = norm(r) + 1e-10\n    1/(8π*μ) * (1/d*I + r*transpose(r)/d^3)\nend\nK = KernelMatrix(G,X,Y)\nH = assemble_hmatrix(K;atol=1e-4)","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"You can now multiply H by a density σ, where σ is a Vector of SVector{3,Float64}","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"σ = rand(SVector{3,Float64},m)\ny = H*σ\n# test the output agains the exact value for a given `i`\ni = 42\nexact = sum(K[i,j]*σ[j] for j in 1:m)\n@assert norm(y[i] - exact) < 1e-4 # hide\n@show y[i] - exact","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"note: Note\nThe naive idea of reinterpreting these matrices of tensors as a (larger) matrix of scalars does not always work because care to be taken when choosing the pivot in the compression stage of the PartialACA in order to exploit some analytic properties of the underlying kernel. See e.g. section 2.3 of this paper for a brief discussion.","category":"page"},{"location":"kernelmatrix/#Vectorized-kernels-and-local-indices","page":"Kernel matrices","title":"Vectorized kernels and local indices","text":"","category":"section"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"A more efficient implementation of your kernel K::AbstractKernelMatrix can sometimes lead to faster assembling times. In particular, providing a permuted kernel Kp using the local indexing system of the HMatrix (and setting the keyword argument global_index=false in assemble_hmatrix) avoids frequent unnecessary index permutations, and can facilitate vectorization. This is because the permuted kernel Kp will be called through Kp[I::UnitRange,J::UnitRange] to fill in the dense blocks of the matrix, through Kp[I::UnitRange,j::int] and adjoint(Kp)[I::UnitRange,j] to build a low-rank approximation of compressible blocks. ","category":"page"},{"location":"kernelmatrix/","page":"Kernel matrices","title":"Kernel matrices","text":"The vectorization example in the Notebook section shows how a custom (and somewhat more complex) implementation of a vectorized Laplace kernel using the LoopVectorization package can lead to faster (sequential) execution.","category":"page"}]
}
